{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 2,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 2,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:50]\n",
    "# integers are the first 50 indeces for each review\n",
    "# 1 is the first word in the review, 14 is the second etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2476])\n",
    "# 279 words in this review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "# trim sequences down so have uniform amount of words across the board. If any are shorter than 80 elements, will padd\n",
    "# with 0's, if longer than 80 elements will cut off. \n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From /Users/maggie/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/maggie/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "WARNING:tensorflow:From /Users/maggie/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 184s 7ms/sample - loss: 0.4612 - acc: 0.7826 - val_loss: 0.4119 - val_acc: 0.8137\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 225s 9ms/sample - loss: 0.2973 - acc: 0.8778 - val_loss: 0.3947 - val_acc: 0.8250\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 192s 8ms/sample - loss: 0.2138 - acc: 0.9187 - val_loss: 0.4144 - val_acc: 0.8374\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 195s 8ms/sample - loss: 0.1495 - acc: 0.9440 - val_loss: 0.4771 - val_acc: 0.8266\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 192s 8ms/sample - loss: 0.1128 - acc: 0.9587 - val_loss: 0.5682 - val_acc: 0.8118\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 206s 8ms/sample - loss: 0.0799 - acc: 0.9717 - val_loss: 0.6170 - val_acc: 0.8262\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 194s 8ms/sample - loss: 0.0552 - acc: 0.9823 - val_loss: 0.7280 - val_acc: 0.8043\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 185s 7ms/sample - loss: 0.0457 - acc: 0.9842 - val_loss: 0.7081 - val_acc: 0.8187\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 183s 7ms/sample - loss: 0.0364 - acc: 0.9877 - val_loss: 0.8019 - val_acc: 0.8161\n",
      "Epoch 10/15\n",
      " 2432/25000 [=>............................] - ETA: 2:40 - loss: 0.0202 - acc: 0.9955"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4860c834adb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     20\u001b[0m score, acc = model.evaluate(x_test, y_test,\n\u001b[1;32m     21\u001b[0m                             batch_size=batch_size)\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# max_features = 20000. most are words, picking based off index location.\n",
    "# embedding layer is not pre-trained model. will take max_features and reduce into a 128 neuron dense vector rep of max_features.\n",
    "# use embedding instead of dense because has wrapper that handles the text. \n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It wasn’t just two exceptional teams and two successful organizations battling each other across Min'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[50][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "# 136 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "text = \" \".join(data)\n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "# create look-up tables:\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences 178374\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # each element is 40 characters long. \n",
    "next_chars = [] # one element (what target is--predicting next character)\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen]) # to give last char in sequence\n",
    "    \n",
    "print('sequences', len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "# don't need to use embedding layer because using characters not words so one-hot encoding will create a relatively small\n",
    "# vector. applicable for this problem, could use embedding with larger problems.\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)# 3D matrix\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178374, 40, 121), (178374, 121))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_int[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = int_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.7868\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"m foreign policy heavyweights in Trump’s\"\n",
      "m foreign policy heavyweights in Trump’s ang and an the the the the the the the the the the the the the the the and the the the the the the and and the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the berere the the the the the the the the the the the the the the the the the the t\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"m foreign policy heavyweights in Trump’s\"\n",
      "m foreign policy heavyweights in Trump’s to the yor th be malg of dand mans the the and for to the were tor tor the rorn the dof cast anle the dad an anen on tor te tho tat ming th reringe rols bicuin on ouls he the tere inf amand the the tie for on the the shan the an ing and the certhe as the tilin the far te the toe ans bond and the  oras the anot un the the the thin the mos ar the the the sfat mant and the wedene sedon tin war or lo\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"m foreign policy heavyweights in Trump’s\"\n",
      "m foreign policy heavyweights in Trump’sif. The coml forkinc, piti,e, bncoaorl aisctire Eas one Piblteryithitiac. Caat. Inpiqhad in the lice the tiatet.\n",
      "\n",
      "outpeve dile tuco hyme therety ralveftor 3rle the tike Tres.t”.\n",
      "\n",
      "TSOtsres bmrata tu sern,ain the wracing fasines fouvo atot angenriT atx atereot ana, ong Bmontra lulciuy, heesint, teliggy thes teas wey phhond sorition somim, acion ppamsibe o” nan Souwr wollite ws ins, peulsstre sakmita\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"m foreign policy heavyweights in Trump’s\"\n",
      "m foreign policy heavyweights in Trump’sl ionMag thivalestRe\n",
      " afy-we (uewse!, Aunflime tit gasvLuwt s galt Dldplgvethans besinos the besven, tiid Tone hooun: te ofw thesinl hossMa-0sny rbrogon hingern vt Cyai me! cacinle.\n",
      "IS, warussit.,Gohes randiv/ugtiTc-. TMa\n",
      "plnodme ffom lf1m ifind ons ong hifrs ce aving ReryizS. ag tBact mon an” yor heeonssanssid 2Af s onid intise ivFsineny atheaur an wediamed in he iD. Mou fhimisiiy 1runt mo EEgs i\n",
      "178374/178374 [==============================] - 264s 1ms/sample - loss: 2.7866\n",
      "Epoch 2/5\n",
      "152960/178374 [========================>.....] - ETA: 33s - loss: 2.3987"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d5794c81093e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[print_callback])\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/U4-S3-Deep-Learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    \n",
    "    print()\n",
    "    print('----generating text after epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('---- generating iwth seed: \"\"' + sentence + '\"\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=1.0)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.2779\n",
      "----generating text after epoch: 0\n",
      "---- generating iwth seed: \"\" showed him body slamming a person with \"\"\n",
      " showed him body slamming a person with anpes. Aut tha “O jerne a bay shesureaud the, seasm.\n",
      "\n",
      "Womn arkery JoCs anderse, ou haien. Dolld. Eulfotiny loug’s’s af the Afbetbomt, goons artes to derosto\n",
      "\n",
      "Ale wher tolatidf nor ien ar ows and Momeriof iaws sho 0 1so on thit shasary adcounacen fout treid soul,” thet of Jo tooms too covists, a erionter if thave. The owing to cespfowisn Cofpenseer. M3Ciqzefor s acerapcer ans toumsens yours somliti\n",
      "178374/178374 [==============================] - 236s 1ms/sample - loss: 2.2777\n",
      "Epoch 2/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.1953\n",
      "----generating text after epoch: 1\n",
      "---- generating iwth seed: \"\"unding for his promised wall along the U\"\"\n",
      "unding for his promised wall along the U.gapo sowrict to owelnts ougher.\n",
      "AD\n",
      "\n",
      "AD\n",
      "I“\n",
      "\n",
      "\n",
      "Turm. incnsters to gngice jond offibegd ol in patil, of the oivests, in resucobat. Shite beal at a taved hhat and the tomume Povo.\n",
      "\n",
      "S Sundecidenin this ouls.”\n",
      "\n",
      "In are iglow Unutice of hark “and dS. Truago eading thes 2510By. Buppers it letile to resvar in mavl of thets wes to soprate ane, th as ald nownredter if The tenweneEm wepprich a tlabsuet, and bi\n",
      "178374/178374 [==============================] - 235s 1ms/sample - loss: 2.1952\n",
      "Epoch 3/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.1313\n",
      "----generating text after epoch: 2\n",
      "---- generating iwth seed: \"\"h quarter)\n",
      "\n",
      "TD for Chargers: The Charger\"\"\n",
      "h quarter)\n",
      "\n",
      "TD for Chargers: The Charger at “Cho pollebinoplisionterated the UPreanet on) I your op the Dcanfivizan seoper poryopien to exevican. “ot Spoalize but of tastacte Ony Snkersces. “The tread har certericed, ialans for hore a cosper Alica. Latter Trummasm a ceny. Wishnt. 2060 Conting of amporeL.\n",
      "\n",
      "Pasturem, the rograng and chading. (Thith of tham Co-aic For sicleneg unding toectuble whoung Thuc tor tee tooty, sien. Indlopht, he \n",
      "178374/178374 [==============================] - 233s 1ms/sample - loss: 2.1313\n",
      "Epoch 4/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.0767\n",
      "----generating text after epoch: 3\n",
      "---- generating iwth seed: \"\"er — as well as the whistleblower’s sour\"\"\n",
      "er — as well as the whistleblower’s sourirsts conblans.” Dempaly. Ox Semeract achunts whon “htalh Shened samer some the goner the Fany Jackey the kext gere sai bost a late Tructery ADporst arsing, the priction zojlecile ip later.\n",
      "\n",
      "Afould jundinn dom. ADe, 4 heS y.n Tire that modic. shisared dogition gomptud Timefar Prumine ofee-laked as imporalid-0.\n",
      "\n",
      "Turmay in chencunfore uppay gepposn to the pald awbyormed it beters, prech oftenties wa\n",
      "178374/178374 [==============================] - 238s 1ms/sample - loss: 2.0767\n",
      "Epoch 5/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.0299\n",
      "----generating text after epoch: 4\n",
      "---- generating iwth seed: \"\",” said Nicole Kaeding, vice president o\"\"\n",
      ",” said Nicole Kaeding, vice president or Aler’s loosmers of thenguces inve commed of ingedpory from that athen mays, bus ‘SI Cayle. 4 te fatantico as his whine clate saulitian eclaly’s sumatorant conlican’s leaded the trete so somesolation sunden sople me band Rievarce.\n",
      "\n",
      "AD\n",
      "\n",
      "A, Chins Sano dlack cabumen brovercent art the was weated whew fameld to rotally disco on Thich Peundays, the reodd cuumarche flabes to hes as by furting that by r\n",
      "178374/178374 [==============================] - 250s 1ms/sample - loss: 2.0300\n",
      "Epoch 6/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9906\n",
      "----generating text after epoch: 5\n",
      "---- generating iwth seed: \"\"the ruling.\n",
      "\n",
      "“In a social and democratic\"\"\n",
      "the ruling.\n",
      "\n",
      "“In a social and democraticy’s morot of the awnol discovice.”\n",
      "\n",
      "Amemfeat to reverctioned thit) Ranked Farny and viop thourge who Gobire.\n",
      "\n",
      "Cows,” and a mightandgy aftia: Howmryading.\n",
      "\n",
      "Af’thing more a Sumpainso strapudely, a signe, makr dimaters chace bet vamy fox greedived partrentake amali2s dorrayle to blebervat concercend anmena-llast preash and lite fill becould or Joumopander “Trive Conment pay allidet.\n",
      "\n",
      "6 assuatains on \n",
      "178374/178374 [==============================] - 237s 1ms/sample - loss: 1.9906\n",
      "Epoch 7/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9539\n",
      "----generating text after epoch: 6\n",
      "---- generating iwth seed: \"\"ear a window. “Perceiving a threat the o\"\"\n",
      "ear a window. “Perceiving a threat the ourd abous dling a Roklic 201 vice, aborions cant for own and. If demains fele of ‘.C.. Bliagnal) Romnas. The look usto kill ew, and isprifo ald cluncres fontonal gomentane the hoss his to plens pusm. Yer sebpsent remplitials own whey in frormed and Morsan, weble” morred fily paine, colland on the leckotrial anes. Hill whed encerts you the wouth. As may ho fleming in chorncous of the crots. Int you\n",
      "178374/178374 [==============================] - 261s 1ms/sample - loss: 1.9539\n",
      "Epoch 8/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9205\n",
      "----generating text after epoch: 7\n",
      "---- generating iwth seed: \"\"vice identifiers, operating system versi\"\"\n",
      "vice identifiers, operating system versing he hass ince mand hervors that saciadser selifo ak a plandem be condoons. “The Climibance Merboizer’s cempinue onituan of Dinlat STiulme a. Softe ise the havourd you’s and nots wher B.-Aplains of Irting”. Bft [EAsumpa Chever. MJ.1Gfre Washingean on Ensuve Hoa’s mojens.\n",
      "\n",
      "·oppore: Jalrs, me Dottacemens, the Arvice forting I rulhing conmints naterclesty over” pals on Inase onlercredst kmote havtra\n",
      "178374/178374 [==============================] - 263s 1ms/sample - loss: 1.9204\n",
      "Epoch 9/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8899\n",
      "----generating text after epoch: 8\n",
      "---- generating iwth seed: \"\"d transcript surfaced in late September.\"\"\n",
      "d transcript surfaced in late September.\n",
      "\n",
      "At 12, ofence, whe hesses.\n",
      "\n",
      "Wat noolly nextot suiddomal opsone is chators. “Ho sugir of the Whick engerrats formationed the timpes. an procest munt the poister of the bonte the Termicaticy ot. Postys. Tom bot in which leeved froms cantens; Natinier and peinly offers buss to stes plowrs soors the C tolwer Twinss,” a desies. Octade whithing bevanf amsumpored. Fords Ferract ow haspowant that coodss\n",
      "178374/178374 [==============================] - 243s 1ms/sample - loss: 1.8899\n",
      "Epoch 10/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8621\n",
      "----generating text after epoch: 9\n",
      "---- generating iwth seed: \"\"… I hope that someday this season we cou\"\"\n",
      "… I hope that someday this season we courive and lat-leage, who has any that heanks, whene Gurtingo; You is ned reas gun plite hecks losk ann3tich of the Unitese nearce reteld and gayoctornial andemire, se7f a the chave of an eiving to be nexple trinstalitigity, en NnCoriay ove the uncusia to plome intoreds mositoees. The mats chankem and Obtriling one neld in to sieviclations ans anden the tweeken who to depeld affertde from the higm s\n",
      "178374/178374 [==============================] - 244s 1ms/sample - loss: 1.8621\n",
      "Epoch 11/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8351\n",
      "----generating text after epoch: 10\n",
      "---- generating iwth seed: \"\"ere’s a good chance the 22-year-old Robl\"\"\n",
      "ere’s a good chance the 22-year-old Roblly, condeds authe/wlo gungit of munt. It’s lews, on the never the ksaids amaup!by.”\n",
      "\n",
      "AD\n",
      "\n",
      "IS and hem into in mealtizess leant aden to your 72. Hender to douncts that for have persaities yaid you canes.” Wurlightens. A-ligetinglo Hounced … We till eatulan frim to the fints, And Mation ooner, curlutian yow insegro feoth piblity. Mutiey bot Broadly and soppoin avers effectrans — use mriim ussider, of \n",
      "178374/178374 [==============================] - 246s 1ms/sample - loss: 1.8351\n",
      "Epoch 12/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8113\n",
      "----generating text after epoch: 11\n",
      "---- generating iwth seed: \"\"its previous four games, actually improv\"\"\n",
      "its previous four games, actually improved sivan’s witt saiver.\n",
      "\n",
      "Marner ABrees)\n",
      "\n",
      "Repubure downty hese capled other noW $35 foots at he lall the pasy of the Dispolicies and “petidation of the enenil on sentriniar out puraction. Rup im notthine.\n",
      "\n",
      "R. fecents a roopies shill whe any head so gak and to told ittine.\n",
      "\n",
      "We hout the sumpordunes minned oghes and somed whether agains stame — ary from theress for just rullat but. You out hout a redi\n",
      "178374/178374 [==============================] - 244s 1ms/sample - loss: 1.8113\n",
      "Epoch 13/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7885\n",
      "----generating text after epoch: 12\n",
      "---- generating iwth seed: \"\"r position is that the American public s\"\"\n",
      "r position is that the American public scesting. The’ve Zolgeald on Octengith 2019 Jenced State Consin, Anementinn Turdia, the Misenter,”, a 255 morch adreds of missubitater Ponrazsion. Ir frim offers of the Aurreminaina in accually villed twhy sig laitial lasida: Stomend, Luadien, equetthior could compacting medions. Then Resskides falt. “At the Eurriquiter. Theratily goange at expearings a stopally actarf corstees LohS’ Dovificasan co\n",
      "178374/178374 [==============================] - 245s 1ms/sample - loss: 1.7885\n",
      "Epoch 14/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7674\n",
      "----generating text after epoch: 13\n",
      "---- generating iwth seed: \"\"so be known as a “check” or “ATM” card a\"\"\n",
      "so be known as a “check” or “ATM” card and Stats (Chice Menjorth,” E-NFC’s States dofing and rrourch of Houlk have The an throupld douce was requencepen: And the peeds of the palts famal tole: Helectia fordoria, Pecoles and Said. Denorgion. Maysownothow 13.\n",
      "\n",
      "9.-4, haspens and racement isseaden me own dormenters pars.\n",
      "\n",
      "We lleapligut. In which we resprods and beend you time, weok had neency, ervortepred to bess, in lef-teered out yanchial\n",
      "178374/178374 [==============================] - 245s 1ms/sample - loss: 1.7675\n",
      "Epoch 15/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7474\n",
      "----generating text after epoch: 14\n",
      "---- generating iwth seed: \"\"omination has given me my career back. A\"\"\n",
      "omination has given me my career back. Appan they he ond wire atpensible suse vicement.\n",
      "\n",
      "“They’e he be age clomm oversund contencement’s sifefund\n",
      "\n",
      "Alty up Grump claine) experts sayed preves, atween. Ot loors and llawain politicull centsies.\n",
      "\n",
      "Hose are toie for a linellagain” and pack beatiast forster as an waye that RePpman Perite offinean, is right to polense was claved theins were with tipa lues that hame to questionding at deconding s\n",
      "178374/178374 [==============================] - 244s 1ms/sample - loss: 1.7474\n",
      "Epoch 16/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7289\n",
      "----generating text after epoch: 15\n",
      "---- generating iwth seed: \"\"rolled out of the heavy mist,” the Times\"\"\n",
      "rolled out of the heavy mist,” the Times to reew’s thind and aliferstant punsed the court stenster frover revoured over them & from. Dy the Dedicin leadion on concusion wile out conting that vidences to on the towe what poited lond abainst Ameine’s saix storbe of the proffic pedsionity.\n",
      "\n",
      "“I was new United Ho donnting the rerection as sutrented. We harm bloith a “enlifoss the veren pered, back fose in Warn OPS with ducks troub of a leage\n",
      "178374/178374 [==============================] - 246s 1ms/sample - loss: 1.7289\n",
      "Epoch 17/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7116\n",
      "----generating text after epoch: 16\n",
      "---- generating iwth seed: \"\"pular streaming service, with more than \"\"\n",
      "pular streaming service, with more than ducknows demants defising he had Ment wele nor her clangext we. “3t Mfan of the added U.C. Garkion a changer at uplecent to nerroen coom kro is sirged that the post in a spimae or mont.\n",
      "\n",
      "No mid oof the idsed (Runder to Asllowance, dusing, allage time ficls went the quistives lows bit sivecauration. Warders Jann Gen Moname, The Badgab Pamps hare gensudgem innold sird would in apwey relowing the evi\n",
      "178374/178374 [==============================] - 274s 2ms/sample - loss: 1.7116\n",
      "Epoch 18/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.6947- ETA: 2s - \n",
      "----generating text after epoch: 17\n",
      "---- generating iwth seed: \"\"lping them.\n",
      "\n",
      "AD\n",
      "\n",
      "AD\n",
      "\n",
      "Currently, about 7,\"\"\n",
      "lping them.\n",
      "\n",
      "AD\n",
      "\n",
      "AD\n",
      "\n",
      "Currently, about 7, a senter opinaty Corviborst wass a quart’ klooking subactralicatic pomit quict of then they cass.\n",
      "\n",
      "The House siverater to that, ressomers not of Forterul wilps pasies. Polton and ylack’s justures firetments. They iddicus for that kee graght in cortionaise of mistriguing itsess placements. horl would be to a lewselted to ond of the haif threigensitiank the’m suce liard that was pricks 5lime a long\n",
      "178374/178374 [==============================] - 273s 2ms/sample - loss: 1.6949\n",
      "Epoch 19/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.6798\n",
      "----generating text after epoch: 18\n",
      "---- generating iwth seed: \"\"s a classic wrestling story line, pittin\"\"\n",
      "s a classic wrestling story line, pitting olleasceplived de a 37. Rournis agpargus over to pibelifullersh” legge the hon Gersect,” the president of I cented to pilas.\n",
      "\n",
      "“Mexile on the NaCuastoun President, he may me of from the United States it heppent full a ratif islould of the rightrep treas. You may needed at herson fortelly by comsoruline, haven beytare but dibigned.\n",
      "\n",
      "Aslind, it our thet’s preable for from to be a report, communchou\n",
      "178374/178374 [==============================] - 274s 2ms/sample - loss: 1.6798\n",
      "Epoch 20/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.6655\n",
      "----generating text after epoch: 19\n",
      "---- generating iwth seed: \"\"e married, you and your spouse could exc\"\"\n",
      "e married, you and your spouse could excluch weeks: Senion eventenfle how his cass goanck. I cooential inteoligers weokles actions with Islicestion wite procts sulvint hakdmsy step uphall out and informe. P.R: Host Jehn hn U.S. frop elest macter maing coantes the Bracal Palksealy fruen adligenton hemp.) — Coons lack un the persoptand that Eurraw. one of the drouged of the thought beatroming agger, Conurg, 22 makely the Wallibian’s Soute\n",
      "178374/178374 [==============================] - 1366s 8ms/sample - loss: 1.6655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x131ad8080>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, \n",
    "         batch_size=128,\n",
    "         epochs=20,\n",
    "         callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "U4-S3-DL (python 3)",
   "language": "python",
   "name": "u4-s3-deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
